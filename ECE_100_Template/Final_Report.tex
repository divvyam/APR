\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{tabto}

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\begin{center}
      \large\textit{CSE 519}\\
      \Large\textbf{Academic Paper Ranking}\\
\end{center}
\section*{Introduction}
Ranking of academic papers helps scholars to focus on their publishing efforts and can
be widely used for judging the quality of work and research done. There have been numerous approaches to rank these papers, the most traditional being peer grading provided by institutions. But these ranks are divergent to be considered as a standard ranking system. There have been many efforts in the past to make the rankings consistent and relative. The ranking metrics now rely on observed bibliometric measures on the journal-level which is used as representative of quality and eliminates the need of subjective assessment. Taking the example of Google Scholar h-index rankings, it avoids many ‘good’ papers just because of the less number of
citations as it has been experimented many times that their algorithm gives citations the most weightage.We would like address this problem by
devising an algorithm based on several factors like impact factors, Altmetrics, its
scientific and social impact.This report documents the progress that we have done in ranking the academic papers. We explored various data sources and experimented with them. The experiments and why or why not that data set is used is explained in the Data Collection section
. The next section introduces the exploration and the pre-processing done on data. Further sections explains the modeling done on the data and evaluation done. The last section concludes the project.

\section*{Data Collection}
% We have used the ACM version 9 data from Citation Data Network for preliminary data analysis. This data was not complete. It was missing many references and author data. We therefore explored another options to get more data:

% \item DBLP : We have explored the DBLP dataset from Citation Network data to integrate with the ACM, but the integration was a pain consider there were duplicate papers and different format of data for both the datasets.
% \item MAG: Of the various datasets explored, we found the MAG data to be highly rich and organised. This paper supports the analysis: http://www.dlib.org/dlib/september16/herrmannova/09herrmannova.html . We are in the process of getting the full data as it is taking time to download the data via API access.
% \item Altmetrics: We had approached the Altmetrics team to get the altmetric data of the academic papers. We got access to the data on November 13 and we are exploring this data set.
We started with exploring the ACM version 9 data from Citation Data Network. Fig 1 explains the dataset. Almost 50\% of the rows didn't have references. Another 5\% of the data had missing author. The missing reference data was handled by filling the references with the values after max reference of the paper.
\begin{figure}[ht]
\centering
  \includegraphics[width=0.7\columnwidth]{ACM_Versio_9_DataSet.png}
  \caption{ACM Version 9}~\label{fig: Microsoft Academic Graph}
\end{figure}
\FloatBarrier
Owning to the missing data in ACM version , we decided to explore the DBLP version 9 data and to merge this data into the ACM dataset. However, the reference id representation in both the data was in an incomparable state and couldn't be merged.
\newline 
To address these issues , we further searched for a better data set and we have come across Microsoft Academic Graph.This dataset is the richest and most comprehensive data till date[1].Fig2 describes the data set that we are downloading from the API . The data was fetched by Microsoft Cognitive Service Labs by using their APIs. The data fetched was in the format of JSON and was converted to CSV.  The limitation here was only 1000 rows can be fetched in a single request to the API. The data was rich in the information as it has categorization of paper, keywords from abstract, proper references, popularity of the paper using Bing's search index. So we went ahead in creating a python script to fetch data from the API. The dataset is rather huge, resulting in million of rows for a single year. To decrease the data download time we used multithreading to concurrently fetch the data.\\
\begin{figure}[ht]
\centering
  \includegraphics[width=0.7\columnwidth]{MAG_data-info.png}
  \caption{Microsoft Academic Graph}~\label{fig: Microsoft Academic Graph}
\end{figure}
\FloatBarrier

Another set of useful dataset is gathered from Scopus. Scopus contains details of all the major conferences around the world and evaluates rank of publications based on different parameters collected.
\begin{figure}[ht]
\centering
  \includegraphics[width=0.7\columnwidth]{Scopus_data_info.png}
  \caption{Microsoft Academic Graph}~\label{fig: Microsoft Academic Graph}
\end{figure}
\FloatBarrier
\section*{Data Exploration on ACM DataSet}
We implemented a function CountReference which counts the number of times a paper is referred. Along with that we calculated outbound references giving us the number of research papers referred by given paper. Then we began analyzing different relations and initial exploratory data analysis. It was observed that number of citation follows Power Law across different research papers. This observation is logical and is in essence the problem with H-index ranking. The papers that are popular gain more traction and keep on getting more citation than other equally good research papers.
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{top_10_papers.png}
        \caption{Number of papers published per year}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{power_law.png}
        \caption{Power Law in action}
    \end{minipage}
\end{figure}
\FloatBarrier
\\

The number of papers published follows a skewed Normal Distribution. The monotonic increment in the number of papers published seems reasonable as the number of domains and research area come up with time. But the decrease in the published paper can only account to improper data collection which neglected the papers published recently in the journal. Another reason behind it can be that the publishing of a paper takes time thus we would be able to see more papers on a later time.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{year-distribution.png}
        \caption{Number of papers published per year}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{year_graph.png}
        \caption{Paper publication year distribution}
    \end{minipage}
\end{figure}
\FloatBarrier
% Put average author graph - TODO
We implemented the reach function which takes a paper as parameter and gives the impact level of the paper. We used DFS to implement the function. 

% PUT GRAPH OF REACH FUNCTION - TODO
\begin{figure}[ht]
  \includegraphics[width=0.6\columnwidth]{Reach.png}
  \caption{Reach of paper indexed 711}~\label{fig:Reach}
\end{figure}
\FloatBarrier
There were some interesting observations.\\

There was one paper with 115 author, this was considered as an outlier since this skewed the average author feature to the right. \\

There were some papers whose reach was as far as \textit{3000} nodes. One of these was  'Basic Concepts in Knowledge Based Systems'
% put the paper name

% \begin{enumerate}
% \item Graph of number of papers vs count of references
% \item Average number of authors of a paper.
% \item One paper with 115 author. interesting?
% \item Co-relation map with page rank?
% \item with geography?
% \end{enumerate}

\section*{Data Exploration and Pre-Processing on MAG DataSet}
\FloatBarrier
Since we had a cleaner and better dataset, we didn't have to do much to clean the data. We first merged the Publication data extracted from the Scopus Dataset. For all the publications that were present in the Scopus data, we merged the SJR rank of the publication with our data. The other publications were dropped. We have 1200 such unique publications. We also extracted a lot of features from the existing data.  The features extracted are :\newline
\begin{itemize}
    \item \textbf{Publication Type:} Along with the Publication Venue, we also had information about the Journal and the Conference of the paper. We assigned different Publication types to the feature based on these two columns.
    \begin{table}[h]
  \centering
  \begin{tabular}{l | r | r}
    % \toprule
    {\small \textit{Journal}}& {\small \textit{Conference}} & {\small \textit{Publication Type}}\\
    {\small \textit{Nan}} & {\small \textit{Present}} & {\small \textit{0}}\\
    {\small \textit{Present}} & {\small \textit{Nan}} & {\small \textit{1}}\\
    {\small \textit{Nan}} & {\small \textit{Nan}} & {\small \textit{3}}\\
    {\small \textit{Present}} & {\small \textit{Present}} & {\small \textit{2}}\\
    % \bottomrule
  \end{tabular}
  \caption{Publication Type}~\label{tab:table1}
\end{table}
\newline
Most of the papers in our data belong to the Journal type as shown in the following graph.
\begin{figure}[ht]
\centering
  \includegraphics[width=0.6\columnwidth]{PublicationType.png}
  \caption{Publication Type}~\label{fig:Reach}
\end{figure}
\FloatBarrier
    \item \textbf{Number Of Authors}: Number of Authors in each paper. The following figure demonstrates the distribution of authors in the dataset.
    \begin{figure}[ht]
    \centering
  \includegraphics[width=0.6\columnwidth]{Number_Of_Authors.png}
  \caption{Authors per paper}~\label{fig:Reach}
\end{figure}
\FloatBarrier
    \item \textbf{First Author} : Since the first author is most influential to the paper, we also extracted this author to be used as a feature
    \item \textbf{Page Rank}: We used this field to do weak supervision on the model. We evaluated the rank of each paper based on the page rank algorithm described in this paper[2].Using the inlinks and outlinks map created, a modified iterative PageRank algorithm is applied to  calculate the authoritative score for each node. In all the iterations, rank of a link is updated using the following formula:
\[ NS = 0.15 + 0.85 * \sum (PR[I]/OC[I]) \]
ie for each link , new score is updated using the number of incoming links and the outgoing links. This score is further normalised using average year citation count. 
    \item \textbf{First Author Rank} After evaluating page rank, we used the rank to score the conferences to further calculate rank of each author. The conference score was evaluated using the following equation:
    \[ CS=\frac{\sum (PS)}{(NPC[C])} \]
    CS = Conference Score , PS = Paper Score, NPC = Number of Papers published in the conference.\newline
    Using the evaluated Conference Score, each author was ranked as follows:
    \[ AS = \frac{\sum (PS*CS)}{NPA[A]} 
    \]
    AS = Author Score, PS = Paper Score, CS = Conference Score, NPA = list of number of papers published by the author
    \item \textbf{Avg Author Rank}: Another feature was extracted from the author based on the author score calculated. The score of all the authors of a paper was averaged to be used as a feature for ranking the paper.
    \item \textbf{Year} : The Year of the publication of paper
\begin{figure}[ht]
\centering
  \includegraphics[width=0.6\columnwidth]{Year_Distribution.png}
  \caption{Distribution of papers published in Year}~\label{fig:Reach}
\end{figure}
\FloatBarrier
    \item \textbf{Month} : Month of Publication of paper.It is interesting to note in the figure that the number of papers published in January are higher than any other month. This is because a large number of conferences are held in this month.
\begin{figure}[ht]
\centering
  \includegraphics[width=0.6\columnwidth]{Month_Distribution.png}
  \caption{Number of paper published in each month}~\label{fig:Reach}
\end{figure}
\FloatBarrier
    \item \textbf{Year Since Publication}: Years elapsed since the publication of paper until current year.
    \item \textbf{Domain Count}: In the dataset, a list of domains associated with the paper were given. The number of domains associated with each paper is thus used as a feature.
    \item \textbf{Domain}: Each paper was associated with one particular domain to be used as a feature. The list of fields given corresponding to each paper were stored in hierarchy of the the generalised domain. Therefore, the last domain was extracted from this list and used as a feature. For the papers, who had a nan in the domain column, scopus data was used to fill in the domain. Since each publication is matched to a domain in the scopus dataset, the nan domains were filled in using this information.
    \begin{figure}[ht]
    \centering
  \includegraphics[width=0.8\columnwidth]{Domain_Dist.png}
  \caption{Domain Distribution}~\label{fig:Reach}
\end{figure}
\FloatBarrier
\end{itemize}

\section*{Reach and Citation Network}

\section*{Data Modelling}
Once the features were extracted, we converted the string features like Venue, Topic to numerical features. The data was then divided into three parts to train and evaluate the model. The training set was created from the papers belonging to Year 2007 to 2010. Papers of Year 2011 were used as validation set.And finally, the model was evaluated on papers from Year 2012 to 2018.\newline 
Top 10 papers after the evaluation:
% Add figure
\newline
Plot of the importance of the features given to the model:\newline
% Add figure

\section*{Evaluation}
After evaluating the rank of the paper, the author score was again evaluated following the equation define above. The plot of comparison of the rankings of top 100 authors is as follows:
% Add figure



\section*{Time Series}


\section*{Conclusion}

% \lipsum[6]



\begin{thebibliography}{9}
\bibitem{Robotics}Drahomira Herrmannova and Petr Knoth \emph{An Analysis of the Microsoft Academic Graph
}
\bibitem{Robotics}Jie Tang, Limin Yao, Duo Zhang, and Jing Zhang. A \emph{ Combination Approach to Web User Profiling. ACM Transactions on Knowledge Discovery from Data (TKDD), (vol. 5 no. 1), Article 2 (December 2010), 44 pages}
\bibitem{Robotics}Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. 2015. \emph{An Overview of Microsoft Academic Service (MAS) and Applications. In Proceedings of the 24th International Conference on World Wide Web (WWW ’15 Companion). ACM, New York, NY, USA, 243-246}

\bibitem{Robotics}Aditya Pratap Singh 
Centre for Data Engineering, IIIT Hyderabad, India
; Kumar Shubhankar ; Vikram Pudi \emph{An efficient algorithm for ranking research papers based on citation network
}

\end{thebibliography}

\end{document}
