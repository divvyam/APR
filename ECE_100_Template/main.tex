\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{tabto}

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\begin{center}
      \large\textit{CSE 519}\\
      \Large\textbf{Academic Paper Ranking}\\
\end{center}
\section*{Introduction}
Ranking of academic papers helps scholars to focus on their publishing efforts and can
be widely used for judging the quality of work and research done. There have been numerous approaches to rank these papers, the most traditional being peer grading provided by institutions. But these ranks are divergent to be considered as a standard ranking system. There have been many efforts in the past to make the rankings consistent and relative. The ranking metrics now rely on observed bibliometric measures on the journal-level which is used as representative of quality and eliminates the need of subjective assessment. Taking the example of Google Scholar h-index rankings, it avoids many ‘good’ papers just because of the less number of
citations as it has been experimented many times that their algorithm gives citations the most weightage.We would like address this problem by
devising an algorithm based on several factors like impact factors, Altmetrics, its
scientific and social impact.This report documents the progress that we have done in ranking the academic papers. We explored various data sources and experimented with them. The experiments and why or why not that data set is used is explained in the Data Collection section
. The next section introduces the exploration and the pre-processing done on data. Further sections explains the modeling done on the data and evaluation done. The last section details the next steps for the project.

\section*{Data Collection}
% We have used the ACM version 9 data from Citation Data Network for preliminary data analysis. This data was not complete. It was missing many references and author data. We therefore explored another options to get more data:

% \item DBLP : We have explored the DBLP dataset from Citation Network data to integrate with the ACM, but the integration was a pain consider there were duplicate papers and different format of data for both the datasets.
% \item MAG: Of the various datasets explored, we found the MAG data to be highly rich and organised. This paper supports the analysis: http://www.dlib.org/dlib/september16/herrmannova/09herrmannova.html . We are in the process of getting the full data as it is taking time to download the data via API access.
% \item Altmetrics: We had approached the Altmetrics team to get the altmetric data of the academic papers. We got access to the data on November 13 and we are exploring this data set.
We started with exploring the ACM version 9 data from Citation Data Network. Almost 50\% of the rows didn't have references. Another 5\% of the data had missing author. The missing reference data was handled by filling the references with the values after max reference of the paper. Merging different data sets was the next step. We tried merging various data sets
%put the graph of NA - TODO 
with this data. The reference id representation in both the data was in an incomparable state. So we dropped the DBLP data.To address these issues , we further searched for a better data set and we have come across Microsoft Academic Graph.This dataset is the richest and most comprehensive data till date[1].Fig1 describes the data set that we are downloading from the API . The data was fetched by Microsoft Cognitive Service Labs by using their APIs. The data fetched was in the format of JSON and was converted to CSV.  The limitation here was only 1000 rows can be fetched in a single request to the API. The data was rich in the information as it has categorization of paper, keywords from abstract, proper references, popularity of the paper using Bing's search index. So we went ahead in creating a python script to fetch data from the API. The dataset is rather huge, resulting in million of rows for a single year. To decrease the data download time we used multithreading to concurrently fetch the data.\\

Another set of useful dataset is gathered from Altmetric and Scopus. Scopus
contains details of all the major conferences around the world and ranks them. This will then be merged with Microsoft Academic data as both the datasets share DOI. The combination of MAG and Altmetric is the final aim. Even after implementing such optimization  we are in process of retrieving complete data. Till we acquire the full data by the APIs, we have made various data prepossessing functions on ACM data which will then be used by the latter data set. 
\begin{figure}[ht]
\centering
  \includegraphics[width=0.7\columnwidth]{MAG_data-info.png}
  \caption{Microsoft Academic Graph}~\label{fig: Microsoft Academic Graph}
\end{figure}
\FloatBarrier

\section*{Data Exploration and Pre-Processing}
We implemented a function CountReference which counts the number of times a paper is referred. Along with that we calculated outbound references giving us the number of research papers referred by given paper. Then we began analyzing different relations and initial exploratory data analysis. It was observed that number of citation follows Power Law across different research papers. This observation is logical and is in essence the problem with H-index ranking. The papers that are popular gain more traction and keep on getting more citation than other equally good research papers.
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{top_10_papers.png}
        \caption{Number of papers published per year}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{power_law.png}
        \caption{Power Law in action}
    \end{minipage}
\end{figure}
\FloatBarrier
\\

The number of papers published follows a skewed Normal Distribution. The monotonic increment in the number of papers published seems reasonable as the number of domains and research area come up with time. But the decrease in the published paper can only account to improper data collection which neglected the papers published recently in the journal. Another reason behind it can be that the publishing of a paper takes time thus we would be able to see more papers on a later time.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{year-distribution.png}
        \caption{Number of papers published per year}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{year_graph.png}
        \caption{Paper publication year distribution}
    \end{minipage}
\end{figure}
\FloatBarrier
This is added as a feature and column in the data. We then used PageRank algorithm[2] to rank a paper by considering year, in degree and out degree of the paper.Using the inlinks and outlinks map created, a modified iterative PageRank algorithm is applied to  calculate the authoritative score for each node. In all the iterations, rank of a link is updated using the following formula:
\[ NS = 0.15 + 0.85 * \sum (PR[I]/OC[I]) \]
ie for each link , new score is updated using the number of incoming links and the outgoing links. This score is further normalised using average year citation count. 
% Put page rank graph - TODO
We also calculated number of authors per paper and trained the model with this feature as well. \\
% Put average author graph - TODO
We implemented the reach function which takes a paper as parameter and gives the impact level of the paper. We used DFS to implement the function. 

% PUT GRAPH OF REACH FUNCTION - TODO
\begin{figure}[ht]
  \includegraphics[width=0.6\columnwidth]{Reach.png}
  \caption{Reach of paper indexed 711}~\label{fig:Reach}
\end{figure}
\FloatBarrier
There were some interesting observations.\\

There was one paper with 115 author, this was considered as an outlier since this skewed the average author feature to the right. \\

There were some papers whose reach was as far as \textit{3000} nodes. One of these was  'Basic Concepts in Knowledge Based Systems'
% put the paper name

% \begin{enumerate}
% \item Graph of number of papers vs count of references
% \item Average number of authors of a paper.
% \item One paper with 115 author. interesting?
% \item Co-relation map with page rank?
% \item with geography?
% \end{enumerate}

\section*{Data Modelling}

% to comment sections out, use the command \ifx and \fi. Use this technique when writing your pre lab. For example, to comment something out I would do:
%  \ifx
%	\begin{itemize}
%		\item item1
%		\item item2
%	\end{itemize}	
%  \fi
The author data and publication venue data was in the form of string and was converted into integers using label encoder so that it can be fed into the model for training. We started with linear regression and trained the model with these features :   Year, Author, Publication Venue, Index id, Count Reference, Number of Authors, Years Since Published, Page Rank. 
The coefficients for the respective features didn't make much sense. Then we used the Light GBM model and the results were pretty much as expected. 


\section*{Evaluation}


It can be seen that the most important feature is the count reference which makes sense as it tells the impact factor of any paper and it should contribute the most in the ranking metric.

\begin{figure}[ht]
  \includegraphics[width=1\columnwidth]{LGBM.jpeg}
  \caption{Importance of features}~\label{fig:Importance of featires}
\end{figure}
\FloatBarrier
Year and Public Venue also has high importance. The number of authors in a paper doesn't has much importance as it doesn't really matter how many authors contributed in a paper. The top 10 papers predicted by this model are \newline
% \begin{center}
\begin{table}[h]
  \centering
  \begin{tabular}{l r}
    % \toprule
    & & \multicolumn{2}{c}{\textbf{Top 10 Research Papers}} \\
    {\small \textit{Distributed temporal video DBMS using vertical class partitioning technique}}& {\small \textit{1.000000}}
      & {\small \textit{Video-on-demand: scalability and QoS control}} & {\small \textit{0.968686}}
      & {\small \textit{Large-scale randomization techniques}} & {\small \textit{0.770289}}
      & {\small \textit{The notion of security for probabilistic cryptosystems}} & {\small \textit{0.714130}}
      & {\small \textit{Migration: a new technique to improve synthesized designs through incremental customization}} & {\small \textit{0.600616}}
      & {\small \textit{Timing and crosstalk driven area routing}} & {\small \textit{0.594996}}
      & {\small \textit{Table-lookup methods for improved performance-driven routing}} & {\small \textit{0.593428}}
      & {\small \textit{A fast fanout optimization algorithm for near-continuous buffer libraries}} & {\small \textit{0.592007}}
      & {\small \textit{Performance driven multi-layer general area routing for PCB/MCM designs}} & {\small \textit{0.585010}}
      & {\small \textit{Buffer insertion for noise and delay optimization}} & {\small \textit{0.559871}}
    % \bottomrule
  \end{tabular}
  \caption{Top Research Papers by pagerank}~\label{tab:table1}
\end{table}


We have taken the top 15 authors generated by our model and plotted their h-index. As we can see, sometimes elements with lower h-index are ranked higher by our model.
% \end{center}
\begin{figure}[ht]
  \includegraphics[width=1\columnwidth]{Picture1.png}
  \caption{Importance of features}~\label{fig:Importance of featires}
\end{figure}
\FloatBarrier

% \lipsum[5]

\section*{Next Steps}
We will start working with the new data being acquired. With the categorization of the data we can give top papers in the domain, top authors in a field. We are planning to use neural network[AutoEncoder/Deep Belief Network] to determine the page rank as mapping a function to a model didn't give the results as expected. We will use the SCOPUS data to rank the publications and use them as a feature for our model. Once we have have perfected our model, we will complete the tasks mentioned in the project requirements like finding top 100, creating citation netwrok etc.

% \lipsum[6]



\begin{thebibliography}{9}
\bibitem{Robotics}Drahomira Herrmannova and Petr Knoth \emph{An Analysis of the Microsoft Academic Graph
}
\bibitem{Robotics}Jie Tang, Limin Yao, Duo Zhang, and Jing Zhang. A \emph{ Combination Approach to Web User Profiling. ACM Transactions on Knowledge Discovery from Data (TKDD), (vol. 5 no. 1), Article 2 (December 2010), 44 pages}
\bibitem{Robotics}Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. 2015. \emph{An Overview of Microsoft Academic Service (MAS) and Applications. In Proceedings of the 24th International Conference on World Wide Web (WWW ’15 Companion). ACM, New York, NY, USA, 243-246}

\bibitem{Robotics}Aditya Pratap Singh 
Centre for Data Engineering, IIIT Hyderabad, India
; Kumar Shubhankar ; Vikram Pudi \emph{An efficient algorithm for ranking research papers based on citation network
}

\end{thebibliography}

\end{document}
